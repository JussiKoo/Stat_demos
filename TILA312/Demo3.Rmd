---
title: "GLM2 Demo 3 R tasks"
author: "Jussi Kauppinen"
date: "`r Sys.Date()`"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 3.4


```{r include=FALSE}
library(nlme)
library(dplyr)
library(tidyr)
library(lattice)
library(lme4)
```
  
```{r }
#a
orth <- Orthodont
head(orth)

#Orthodont contains data of change in orthodontic measurement 
#over time for several young subjects. 
#One row contains a distance measurement, age, Subject and Sex.

#b

#How many subjects dataset contains
subjectcount <- n_distinct(orth$Subject)
subjectcount

#How many measurements there are per subject
nrow(orth)/subjectcount

#c

xyplot(orth$distance ~ orth$age|orth$Subject, data=orth, type="b")

xyplot(orth$distance ~ orth$age | Sex, group = Sex, data=orth, type="b", as.table=T, layout=c(2,1))

xyplot(orth$distance ~ orth$age, group = Subject, data=orth, type="b", as.table=T)

#d

#data is now in long format

orthw <- pivot_wider(data = orth, names_from = age, values_from = distance)

#e

meansM <- aggregate(orth$distance, list(orth$age, orth$Sex), mean)

xyplot(meansM$Group.1~meansM$x, col=meansM$Group.2)

```

Linear model is justifiable since the mean distance seems to raise linearly for both sexes.
It also seems like the rate of change is different between sexes so interaction term would also be justifiable.

```{r}
#f

orth$age_c <- orth$age-min(orth$age)
attach(orth)

#g

mm1 <- gls(distance ~ age_c*Sex, data=orth, method="ML")
summary(mm1)
```
$\beta_0$ is 22,62 which means that 8-year old male subject has distance of 22,6 mm.
$\beta_1$ is 0,78 so for males 1 year makes a 0,78 mm difference in distance.
$\beta_2$ is -1,41 so for 8-year old female subject has distance of 22,62 mm - 1,41 mm = 21,21 mm
$\beta_3$ is -0,30 so so for female subjects 1 year makes a 0,78 mm - 0,30 mm = 0,48 mm difference in distance.

# Task 3.5

```{r}
#h

#Let's assume compound symmetry for observations of the same subject
mm2 <- gls(distance ~ age_c*Sex, data=orth,
           correlation = corCompSymm(form = ~ 1 | Subject),
           method="ML")
summary(mm2)

```

Residuals are pretty much the same looking at the standardized residuals and
residual standard errors. There is some dependency since $\rho \approx 0,62$.

```{r}
#i

mm3 <- gls(distance ~ age_c*Sex, data=orth,
           correlation = corAR1(form = ~ 1 | Subject),
           method="ML")
summary(mm3)

```

$\phi \approx 0,61$ so dependency is slightly smaller. Residuals seem to be a bit different looking at standardized residuals.

```{r}
#j

#mm1 and mm2 are nested. mm1 and mm3 are nested. mm2 and mm3 are not nested.
#Nested models can be compared with likelihood ratio.

anova(mm1, mm2)
anova(mm1, mm3)
anova(mm2, mm3)

#mm2 is the best model looking at the AIC-values.
```
```{r}
#k

#Let's assume compound symmetry for observations of the same subject
mm2reml <- gls(distance ~ age_c*Sex, data=orth,
           correlation = corCompSymm(form = ~ 1 | Subject),
           method="REML")
s2 <- summary(mm2reml)
s2

```

Standard errors of coefficients changed a bit. Also the residuals changed a bit.
Value of $\rho$ also slightly raised. 
Restricted maximum likelihood estimation corrects the bias of covariance estimates.

```{r}

#l

betahat <- coef(mm2reml)
betahat

p <- length(betahat)
n <- length(distance)

sdbetahat <- sqrt(diag(s2$varBeta))
sdbetahat

#Hypothesis: Beta1 is 0.8
#t-statistic

t <- (betahat[2] - 0.8)/sdbetahat[2]
t

1-pt(t, n-p-1)

#Hypothesis can't be rejected.

#Hypothesis: Beta1 + Beta3 is 0.5
#Define model matrix

X <- model.matrix(mm2reml)

F_test <- function(K, X, beta, m, sigma)
{
  q <- qr(K)$rank; n <- nrow(X); p <- ncol(X)
  C <- solve(t(K) %*% solve(t(X) %*% X) %*% K)
  F <- t(t(K) %*% beta - m) %*% C %*% (t(K) %*% beta - m) / (q * sigma)
  p_val <- 1 - pf(F, q, n - p - 1)
  list(F = F, p_val = p_val)
}

K <- cbind(c(0,1,0,1))
m <- c(0.5)

#I couldn't figure out how to test this in R. I don't think the F-test
#provided in the last demo works here.


```
## Task 3.6

```{r}
#a

pisa <- read.table("http://users.jyu.fi/~knordhau/GLM2/pisafull.txt", header=TRUE)
head(pisa)
attach(pisa)

#b

gendermeans <- aggregate(sciescore, list(matem=matem,gender=gender), data=pisa, mean)
gendermeans

xyplot(gendermeans$x ~ gendermeans$matem | gendermeans$gender, type="b")

motivmeans <- aggregate(sciescore, list(matem=matem, motiv=motiv), mean)
motivmeans

xyplot(motivmeans$x ~ motivmeans$matem | motivmeans$motiv, type="b")

xyplot(sciescore ~ ESCS)

schoolmeans <- aggregate(sciescore, list(SCHOOLID=SCHOOLID), data=pisa, mean)

regionmeans <- aggregate(sciescore, list(matem=matem, region=region), data=pisa, mean)
regionmeans

xyplot(regionmeans$x ~ regionmeans$matem | regionmeans$region, type="b")

summary(schoolmeans$x)
sd(schoolmeans$x)

```
There seems to be linearity between science score and math grade.
Students with higher motivation seem to get higher scores whereas scores between
boys and girls seem to be very similar in this data. ESCS also seems to have positive
effect on science score. There also seems to be lots of variation in the mean scores
between schools. Region doesn't seem to make a huge difference.

```{r}
#c and d

library(nlme)

fit1 <- gls(sciescore ~ matem + motiv + gender + ESCS + urban + region,
            correlation = corCompSymm(form = ~ 1 | SCHOOLID),
            method="ML")
summary(fit1)

```
## Task 3.7

```{r}
#f

#reduced model
fit2 <- gls(sciescore ~ matem + gender + ESCS + motiv + urban,
            correlation = corCompSymm(form = ~ 1 | SCHOOLID),
            method="ML")

#f-test

anova(fit1, fit2)

```

p-value would indicate now that the region is not needed in the model. Doing the
similar F-test for other variables we say that matem, ESCS and motiv are
significant. Interaction between matem and motiv is needed. I used anova to investigate significance of
interactions.

```{r}
#g

fit <- gls(sciescore ~ matem * motiv + ESCS,
            correlation = corCompSymm(form = ~ 1 | SCHOOLID),
            method="ML")
summary(fit)

#i

#remove the correlation to make it a regular linear model

fitreg <- gls(sciescore ~ matem * motiv + ESCS,
            method="ML")

anova(fit, fitreg)

```

general model has lower AIC so it's better.


```{r}
#j

fitREML <- gls(sciescore ~ matem * motiv + ESCS,
            correlation = corCompSymm(form = ~ 1 | SCHOOLID),
            method="REML")
summary(fitREML)

#Coefficient standard errors are slightly lower.

```
## Task 3.8

```{r}

#a

#hierarchial model with single random effect
fit1 <- lme(sciescore ~ matem * motiv + ESCS, random = ~ 1 | SCHOOLID, data=pisa)

#generalized linear model with compound symmetry structure
#for correlation
fit2 <- gls(sciescore ~ matem * motiv + ESCS,
correlation = corCompSymm(form = ~ 1 | SCHOOLID), data = pisa)

summary(fit1)
summary(fit2)
anova(fit1, fit2)
```
Model estimates and their standard errors are identical. Anova also doesn't make a difference between models.


```{r}

#c

#random effects for schools
ref <- random.effects(fit1)

plot(ref)

```
