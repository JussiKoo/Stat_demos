---
title: "Assignment"
author: "Jussi Kauppinen 2891068547"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 1

a) 

The reason why normal distribution is so special in context of general linear models is statistical inference. Usually in case of general linear models the confidence intervals and hypothesis testing for the model parameters are important parts of the process. In cases where random variables error terms follow the normal distribution with same variance the exact distribution for the MLE of parameter vector can be determined. The MLE is actually normally distributed. This makes statistical inference simple. In cases where variances are not constant similar results arise. In cases where error terms are not normally distributed the sampling distributions can be obtained with asymptotic distributions. For large samples the standardized estimator can be approximated by normal distribution.

b) Probability density function of the $\chi^2$-distribution is

\begin{equation}
\begin{split}
f(y, k) &= \frac{1}{2^{k/2} \Gamma(k/2)} y^{k/2 - 1} e^{-y/2}\\
&= \text{exp} \left( \log \left( \frac{1}{2^{k/2} \Gamma(k/2)} y^{k/2 - 1} e^{-y/2} \right) \right)\\
&= \text{exp} \left( - \frac{k}{2} \log 2 - \log \Gamma(\frac{k}{2}) + (\frac{k}{2} - 1) \log y - \frac{y}{2} \right)\\
&= \text{exp} \left( (\frac{k}{2} - 1) \log y - (\frac{k}{2} \log 2 + \log \Gamma(\frac{k}{2})) -\frac{y}{2} \right)\\
\end{split}
\end{equation}

Let's set $\theta = \frac{k}{2}-1$, $b(\theta) = (\theta+1) \log2 + \log \Gamma(\theta+1)$, $c(y, \phi) = -\frac{y}{2}$, $\phi = 1$ and $a(\phi)=1$. Now we can see that $\log y$ belongs to exponential family. Canonical link is $\theta = \frac{k}{2}-1$. I think GLM might not include this distribution because of the 


# Task 2

Hurdle model consists of two components. One component for positive counts such as Poisson model and one hurdle component models if the count is zero vs larger. The hurdle models probability density function is

\begin{equation}
f_{hurdle}(y;x,z,\beta, \gamma) = 
\begin{cases}
f_{zero}(0;x,\gamma) &, \quad \text{if} y=0\\
(1-f_{zero}(0;x,\gamma)) \frac{f_{count}(y;x,\beta)}{1-f_{count}(0;x,\beta)} &, \quad \text{if} y>0\\
\end{cases}
\end{equation}

Basically the 

Let's choose some example model from the data. It seems like mountain top removal mining affects the number of salamanders on sites. Also the amount of cover objects also seems to have an effect. It can also be seen that there's lots of zero-observations which calls for hurdle model.

```{r include=FALSE}
library(glmmTMB)
library(pscl)
library(ggplot2)
library(lattice)
library(mltest)
```

```{r}
salam <- Salamanders
attach(salam)

plot(table(count))

#Mining's effect on the observed number of salamanders
plot(count ~ mined)

#Covers effect on the observed number of salamanders
plot(count ~ cover)
```

```{r}
#Regular Poisson model
fit <- glm(count ~ mined + cover, data = salam, family="poisson")
summary(fit)

#Hurdle model
fith <- hurdle(count ~ mined + cover, data = salam,
               dist="poisson", zero.dist="binomial", link="logit")
summary(fith)
```

Let's compare maximum log-likelihoods of the models and compare what kind to estimations both models give for 

```{r}
#AIC for both models
AIC(fit, fith)

#Compare log-likelihoods
logLik(fit)
logLik(fith)

#Expected number of zero-observations from both models and the actual number.
sum(dpois(0, fitted(fit)))
sum(predict(fith, type="prob")[,1])
sum(as.numeric(salam$count==0))
```

We can see from all this criteria that hurdle model is better in this case.

For the interpretations here the hurdle component basically gives the probability to encounter lizards. The count component models the positive counts of lizards.


# Task 3
