---
title: "Assignment"
author: "Jussi Kauppinen 2891068547"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Task 1

a) 

The reason why normal distribution is so special in context of general linear models is statistical inference. Usually in case of general linear models the confidence intervals and hypothesis testing for the model parameters are important parts of the process. In cases where random variables error terms follow the normal distribution with same variance the exact distribution for the MLE of parameter vector can be determined. The MLE is actually normally distributed. This makes statistical inference simple. In cases where variances are not constant similar results arise. In cases where error terms are not normally distributed the sampling distributions can be obtained with asymptotic distributions. For large samples the standardized estimator can be approximated by normal distribution.

b) Probability density function of the $\chi^2$-distribution is

\begin{equation}
\begin{split}
f(y, k) &= \frac{1}{2^{k/2} \Gamma(k/2)} y^{k/2 - 1} e^{-y/2}\\
&= \text{exp} \left( \log \left( \frac{1}{2^{k/2} \Gamma(k/2)} y^{k/2 - 1} e^{-y/2} \right) \right)\\
&= \text{exp} \left( - \frac{k}{2} \log 2 - \log \Gamma(\frac{k}{2}) + (\frac{k}{2} - 1) \log y - \frac{y}{2} \right)\\
&= \text{exp} \left( (\frac{k}{2} - 1) \log y - (\frac{k}{2} \log 2 + \log \Gamma(\frac{k}{2})) -\frac{y}{2} \right)\\
\end{split}
\end{equation}

Let's set $\theta = \frac{k}{2}-1$, $b(\theta) = (\theta+1) \log2 + \log \Gamma(\theta+1)$, $c(y, \phi) = -\frac{y}{2}$, $\phi = 1$ and $a(\phi)=1$. We can see that this is almost fulfills the form of exponential family. Only thing is that we have $\log y$ instead of $y$. Apparently this is sufficient and $\chi^2$-distribution is a member of exponential family. Canonical link is $\theta = \frac{k}{2}-1$. However the form of the distribution yields following results:

$$E(\log(y)) = b'(\theta)$$
$$Var(\log(y)) = a(\phi)b''(\theta)$$
I think this might be the reason why glm-function in R doesn't support $\chi^2$-distribution.


# Task 2

Hurdle model consists of two components. One component for positive counts such as Poisson model and one hurdle component models if the count is zero vs larger. The hurdle models probability density function is

\begin{equation}
f_{hurdle}(y;x,z,\beta, \gamma) = 
\begin{cases}
f_{zero}(0;x,\gamma) &, \quad \text{if} y=0\\
(1-f_{zero}(0;x,\gamma)) \frac{f_{count}(y;x,\beta)}{1-f_{count}(0;x,\beta)} &, \quad \text{if} y>0\\
\end{cases}
\end{equation}

Basically the 

Let's choose some example model from the data. It seems like mountain top removal mining affects the number of salamanders on sites. Also the amount of cover objects also seems to have an effect. It can also be seen that there's lots of zero-observations which calls for hurdle model.

```{r include=FALSE}
library(glmmTMB)
library(pscl)
library(ggplot2)
library(lattice)
library(mltest)
```

```{r}
salam <- Salamanders
attach(salam)

plot(table(count))

#Mining's effect on the observed number of salamanders
plot(count ~ mined)

#Covers effect on the observed number of salamanders
plot(count ~ cover)
```

```{r}
#Regular Poisson model
fit <- glm(count ~ mined + cover, data = salam, family="poisson")
summary(fit)

#Hurdle model
fith <- hurdle(count ~ mined + cover, data = salam,
               dist="poisson", zero.dist="binomial", link="logit")
summary(fith)
```

Let's compare maximum log-likelihoods of the models and compare what kind to estimations both models give for 

```{r}
#AIC for both models
AIC(fit, fith)

#Compare log-likelihoods
logLik(fit)
logLik(fith)

#Expected number of zero-observations from both models and the actual number.
sum(dpois(0, fitted(fit)))
sum(predict(fith, type="prob")[,1])
sum(as.numeric(salam$count==0))
```

We can see from all this criteria that hurdle model is better in this case.

For the interpretations here the hurdle component basically gives the probability to encounter lizards. The count component models the positive counts of lizards.

# Task 3

```{r}
discharge <- read.table("discharge.txt", header=TRUE, sep=";")

set.seed(8547)

#sample indexes
s <- sample(nrow(discharge), 850)

#Split discharge data into modeling data and test data
data <- discharge[s,]
test <- discharge[-s,]

#Shift indexes to 0 and 1
data$death <- data$death - 1
data$gender <- data$gender - 1
data$inh_inj <- data$inh_inj - 1
data$flame <- data$flame - 1
data$race <- data$race - 1
data$gender <- factor(data$gender, levels=c(0,1), labels=c("Female", "Male"))
data$inh_inj <- factor(data$inh_inj, levels=c(0,1), labels=c("No", "Yes"))
data$flame <- factor(data$flame, levels=c(0,1), labels=c("No", "Yes"))
data$race <- factor(data$race, levels=c(0,1), labels=c("Non-white", "White"))

#Convert age and tbsa to numeric values
data$age <- as.numeric(gsub(",", ".", data$age))
data$tbsa <- as.numeric(gsub(",", ".", data$tbsa))

#New variable where age is centered to mean age
data$c_age <- data$age - mean(data$age)

attach(data)

#Compute ratio of deaths vs cases for each age. Ages are rounded to whole numbers.
dratioage <- aggregate(death, by=list(round(age)), FUN=mean)

ggplot(data=dratioage, aes(x=Group.1, y=x))+
  geom_point()+
  xlab("Age")+
  ylab("Mortality rate")+
  theme_bw()
```
Effect of age to probability of dying seems to be relatively linear.

Next: similarly effect of total burn area to probability of dying.

effect of inhalation injury to probability of dying.

```{r}
dratioburn <- aggregate(death, by=list(round(tbsa)), FUN=mean)

ggplot(data=dratioburn, aes(x=Group.1, y=x))+
  geom_point()+
  xlab("Total burn area")+
  ylab("Mortality rate")+
  theme_bw()
```

Total surface burn area has obvious effect on probability of dying. We can notice that almost all the patients died after burn area exceeds certain limit.

```{r}
dratioinh <- aggregate(death, by=list(inh_inj), FUN=mean)

ggplot(data=dratioinh, aes(x=as.factor(Group.1), y=x))+
  geom_bar(stat="identity")+
  xlab("Burn involved inhalation injury")+
  ylab("Mortality rate")+
  ylim(0,1)
```
Inhalation injury has major effect on mortality rate.

```{r}
dratioflame <- aggregate(death, by=list(flame), FUN=mean)

ggplot(data=dratioflame, aes(x=as.factor(Group.1), y=x))+
  geom_bar(stat="identity")+
  xlab("Flame involved in burn injury")+
  ylab("Mortality rate")+
  ylim(0,1)
```
Flame being involved in injury has some effect too.

```{r}
dratiogender <- aggregate(death, by=list(gender), FUN=mean)

ggplot(data=dratiogender, aes(x=as.factor(Group.1), y=x))+
  geom_bar(stat="identity")+
  xlab("Gender")+
  ylab("Mortality rate")+
  ylim(0,1)

dratiorace <- aggregate(death, by=list(race), FUN=mean)

ggplot(data=dratiorace, aes(x=as.factor(Group.1), y=x))+
  geom_bar(stat="identity")+
  xlab("Race")+
  ylab("Mortality rate")+
  ylim(0,1)
```
Gender and race seems to have little to no effect.

Let's begin by fitting a model with all variables.

```{r}
fitall <- glm(death ~ age + gender + race + tbsa + inh_inj + flame, data=data, family="binomial")
summary(fitall)
```
Significance of gender and race is in line with what we saw earlier so they are dropped from the model. Flame being included in burn injury seemed to be somewhat relevant earlier but now it doesn't seem to be significant. It is dropped as well.

Let's consider interactions between remaining variables.

```{r}
fit <- glm(death ~ age * inh_inj + tbsa, data=data, family="binomial")
summary(fit)
```



